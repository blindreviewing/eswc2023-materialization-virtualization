#!/usr/bin/env python3

import os
import sys
import json
import unittest
import requests
import warnings
import psycopg2
import pymysql
import subprocess
import docker
import atexit
from glob import glob
from container import Container, ContainerManager
from rmlmapper import RMLMapper
from postgresql import PostgreSQL
from mysql import MySQL
from virtuoso import Virtuoso
from morphkgc import MorphKGC
from sdmrdfizer import SDMRDFizer
from ontop import OntopMaterialize, OntopVirtualize
from fuseki import Fuseki
from query import Query
from executor import Executor
from time import sleep
from rdflib import Graph

DATA_DIR = os.path.join(os.path.dirname(__file__), 'data')
CONFIG_DIR = os.path.join(os.path.dirname(__file__), 'config')
LOGS_FILE_NAME = 'logs.txt'
METRICS_FILE_NAME = 'metrics.jsonl'
CHECKPOINT_FILE_NAME = '.done'
EXIT_CODE_INTERRUPTED = -4
METRIC_TYPE_STOP = 'STOP'
METRIC_TYPE_START = 'START'
METRIC_TYPE_INIT = 'INIT'
METRIC_TYPE_MEASUREMENT = 'MEASUREMENT'
METRIC_TYPE_EXIT = 'EXIT'

manager = ContainerManager()

@atexit.register
def clean():
    if len(manager.list_all()) > 0:
        print('Stopping all containers before exiting...')
        manager.stop_all()

    for f in glob(f'{os.path.join(DATA_DIR, "shared")}/*'):
        # Keep original data and mappings
        if 'student' in f or 'mapping' in f or f.endswith('.q') \
           or f.endswith('.sql'):
            continue
        os.remove(f)

class UnitTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def tearDownClass(self):
        manager.stop_all()

    def test_docker_run(self):
        c = Container('nginx:alpine', 'test_docker_run', False,
                      {'80/tcp': '8080/tcp'})
        self.assertTrue(c.run())
        sleep(5)
        r = requests.get('http://localhost:8080')
        self.assertEqual(r.status_code, 200)
        r.raise_for_status()
        c.stop()

    def test_docker_run_and_wait_for_log(self):
        c = Container('nginx:alpine', 'test_docker_run_and_wait_for_log', False,
                      {'80/tcp': '8081/tcp'})
        self.assertTrue(c.run_and_wait_for_log('start worker process'))
        r = requests.get('http://localhost:8081')
        self.assertEqual(r.status_code, 200)
        r.raise_for_status()
        c.stop()

    def test_docker_run_and_wait_for_exit(self):
        c = Container('alpine:edge', 'test_docker_run_and_wait_for_exit', False,
                      {'80/tcp': '8082/tcp'})
        self.assertTrue(c.run_and_wait_for_exit('sleep 5'))
        c.stop()

    def test_docker_logs(self):
        c = Container('nginx:alpine', 'test_docker_logs', False,
                      {'80/tcp': '8083/tcp'})
        self.assertTrue(c.run_and_wait_for_log('start worker process'))
        self.assertIsNotNone(c.logs())
        self.assertTrue(len(c.logs()) > 0)
        c.stop()

    def test_docker_stats(self):
        c = Container('nginx:alpine', 'test_docker_stats', False,
                      {'80/tcp': '8084/tcp'})
        self.assertTrue(c.run_and_wait_for_log('start worker process'))
        stats1 = c.stats()

        # Trigger some activity
        for i in range(5):
            sleep(i)
            r = requests.get('http://localhost:8084')
            self.assertEqual(r.status_code, 200)
            r.raise_for_status()

        # Stats should be increased by now
        stats2 = c.stats()
        cpu1 = stats1['cpu_total_time']
        cpu2 = stats2['cpu_total_time']
        self.assertGreater(cpu2, cpu1)
        cpu1 = stats1['cpu_user_time']
        cpu2 = stats2['cpu_user_time']
        self.assertGreaterEqual(cpu2, cpu1)
        cpu1 = stats1['cpu_system_time']
        cpu2 = stats2['cpu_system_time']
        self.assertGreaterEqual(cpu2, cpu1)
        mem1 = stats1['memory_total_size']
        mem2 = stats2['memory_total_size']
        self.assertNotEqual(mem1, mem2)

        c.stop()

    def test_postgresql(self):
        postgresql = PostgreSQL(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(postgresql.initialization())
        self.assertTrue(postgresql.wait_until_ready())
        connection = psycopg2.connect(host='localhost', database='db',
                                      user='root', password='root')
        cursor = connection.cursor()

        # Test valid query
        cursor.execute('SELECT 1;')

        # Test load CSV
        self.assertTrue(postgresql.load('student.csv', 'student'))
        cursor.execute('SELECT name, age FROM student;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Test invalid query
        with self.assertRaises(psycopg2.errors.UndefinedTable):
            cursor.execute('SELECT * FROM INVALID_TABLE;')
        connection.rollback()

        connection.close()
        postgresql.stop()

        # Check if the tables are really dropped
        postgresql = PostgreSQL(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(postgresql.wait_until_ready())
        connection = psycopg2.connect(host='localhost', user='root',
                                      password='root', database='db')
        cursor = connection.cursor()
        with self.assertRaises(psycopg2.errors.UndefinedTable):
            cursor.execute('SELECT name, age FROM student;')
        connection.rollback()

        # Check if we can now reload
        self.assertTrue(postgresql.load('student.csv', 'student'))
        cursor.execute('SELECT name, age FROM student;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Check SQL schema loading with 2 tables
        CSV_FILES = [('student.csv', 'student1'),('student.csv', 'student2')]
        self.assertTrue(postgresql.load_sql_schema('schema_postgresql.sql',
                                                   CSV_FILES))
        # Verify table 1
        cursor.execute('SELECT name, age FROM student1;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Verify table 2
        cursor.execute('SELECT name, age FROM student2;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        connection.close()
        postgresql.stop()

    def test_mysql(self):
        mysql = MySQL(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(mysql.initialization())
        self.assertTrue(mysql.wait_until_ready())
        connection = pymysql.connect(host='localhost', user='root',
                                     password='root', db='db')
        cursor = connection.cursor()

        # Test valid query
        cursor.execute('SELECT 1;')

        # Test load CSV
        self.assertTrue(mysql.load('student.csv', 'student'))
        cursor.execute('SELECT name, age FROM student;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Test invalid query
        with self.assertRaises(pymysql.err.ProgrammingError):
            cursor.execute('SELECT * FROM INVALID_TABLE;')

        # Close connection and stop container to drop all created tables
        connection.close()
        mysql.stop()

        # Check if the tables are really dropped
        mysql = MySQL(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(mysql.wait_until_ready())
        connection = pymysql.connect(host='localhost', user='root',
                                     password='root', db='db')
        cursor = connection.cursor()
        with self.assertRaises(pymysql.err.ProgrammingError):
            cursor.execute('SELECT name, age FROM student;')

        # Check if we can now reload
        self.assertTrue(mysql.load('student.csv', 'student'))
        cursor.execute('SELECT name, age FROM student;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Check SQL schema loading with 2 tables
        CSV_FILES = [('student.csv', 'student1'),('student.csv', 'student2')]
        self.assertTrue(mysql.load_sql_schema('schema_mysql.sql', CSV_FILES))

        # Recreate cursor because we dropped tables
        connection.close()
        connection = pymysql.connect(host='localhost', user='root',
                                     password='root', db='db')
        cursor = connection.cursor()

        # Verify table 1
        cursor.execute('SELECT name, age FROM student1;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        # Verify table 2
        cursor.execute('SELECT name, age FROM student2;')
        results = []
        for record in cursor:
            results.append([record[0], record[1]])
        expected = [['Jefke', '21'], ['Maria', '22'], ['Jos', '23'], [None, None]]
        self.assertListEqual(results, expected)

        connection.close()
        mysql.stop()

class FileTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def tearDownClass(self):
        manager.stop_all()

    def test_rmlmapper_file(self):
        rmlmapper = RMLMapper(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'rmlmapper_file.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(rmlmapper.execute_mapping('mapping.rml.ttl',
                                                  'rmlmapper_file.nt',
                                                  'ntriples'))
        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'rmlmapper_file.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'rmlmapper_file.nt'),
                format='ntriples')
        # 3 triples and 1 mapped as NULL <predicate> NULL because NULL values 
        # are undefined in CSV.
        self.assertEqual(len(g), 4)
        rmlmapper.stop()

    def test_morphkgc_file(self):
        morphkgc = MorphKGC(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'morphkgc', 'shared',
                                   'morphkgc_file.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(morphkgc.execute_mapping('mapping.rml.ttl',
                                                 'morphkgc_file.nt',
                                                 'ntriples'))
        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'morphkgc_file.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'morphkgc_file.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)
        morphkgc.stop()

    def test_sdmrdfizer_file(self):
        sdmrdfizer = SDMRDFizer(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'sdmrdfizer_file.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(sdmrdfizer.execute_mapping('mapping.rml.ttl',
                                                   'sdmrdfizer_file.nt',
                                                   'ntriples'))
        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'sdmrdfizer_file.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared',
                             'sdmrdfizer_file.nt'), format='ntriples')
        self.assertEqual(len(g), 3)
        sdmrdfizer.stop()


class RDBTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def setUpClass(cls):
        cls._mysql = MySQL(DATA_DIR, CONFIG_DIR, False)
        cls._mysql.wait_until_ready()
        cls._mysql.load('student.csv', 'student')

        cls._postgresql = PostgreSQL(DATA_DIR, CONFIG_DIR, False)
        cls._postgresql.wait_until_ready()
        cls._postgresql.load('student.csv', 'student')

    @classmethod
    def tearDownClass(cls):
        cls._mysql.stop()
        cls._postgresql.stop()
        manager.stop_all()

    def test_rmlmapper_mysql(self):
        rmlmapper = RMLMapper(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'rmlmapper_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(rmlmapper.execute_mapping('mapping.r2rml.ttl',
                                                  'rmlmapper_mysql.nt',
                                                  'ntriples', 'root', 'root',
                                                  'MySQL', '3306', 'db',
                                                  'MySQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'rmlmapper_mysql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'rmlmapper_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        rmlmapper.stop()

    def test_rmlmapper_postgresql(self):
        rmlmapper = RMLMapper(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'rmlmapper_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(rmlmapper.execute_mapping('mapping.r2rml.ttl',
                                                  'rmlmapper_postgresql.nt',
                                                  'ntriples', 'root', 'root',
                                                  'PostgreSQL', '5432', 'db',
                                                  'PostgreSQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'rmlmapper_postgresql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'rmlmapper_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        rmlmapper.stop()

    def test_morphkgc_mysql(self):
        morphkgc = MorphKGC(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'morphkgc_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(morphkgc.execute_mapping('mapping.r2rml.ttl',
                                                 'morphkgc_mysql.nt',
                                                 'ntriples', 'root', 'root',
                                                 'MySQL', '3306', 'db',
                                                 'MySQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'morphkgc_mysql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'morphkgc_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        morphkgc.stop()

    def test_morphkgc_postgresql(self):
        morphkgc = MorphKGC(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'morphkgc_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(morphkgc.execute_mapping('mapping.r2rml.ttl',
                                                 'morphkgc_postgresql.nt',
                                                 'ntriples', 'root', 'root',
                                                 'PostgreSQL', '5432', 'db',
                                                 'PostgreSQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'morphkgc_postgresql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'morphkgc_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        morphkgc.stop()

    def test_sdmrdfizer_mysql(self):
        sdmrdfizer = SDMRDFizer(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared', 'sdmrdfizer_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(sdmrdfizer.execute_mapping('mapping.r2rml.ttl',
                                                   'sdmrdfizer_mysql.nt',
                                                   'ntriples', 'root', 'root',
                                                   'MySQL', '3306', 'db',
                                                   'MySQL'))

        self.assertTrue(os.path.exists(os.path.join(DATA_DIR, 'shared',
                                                    'sdmrdfizer_mysql.nt')))
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'sdmrdfizer_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        sdmrdfizer.stop()

    def test_sdmrdfizer_postgresql(self):
        sdmrdfizer = SDMRDFizer(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'sdmrdfizer_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(sdmrdfizer.execute_mapping('mapping.r2rml.ttl',
                                                   'sdmrdfizer_postgresql.nt',
                                                   'ntriples', 'root', 'root',
                                                   'PostgreSQL', '5432', 'db',
                                                   'PostgreSQL'))

        exist = os.path.exists(os.path.join(DATA_DIR, 'shared',
                                            'sdmrdfizer_postgresql.nt'))
        self.assertTrue(exist)
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'sdmrdfizer_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        sdmrdfizer.stop()

    def test_ontopmaterialize_mysql(self):
        ontop = OntopMaterialize(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'ontopmaterialize_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(ontop.execute_mapping('mapping.r2rml.ttl',
                                              'ontopmaterialize_mysql.nt',
                                              'ntriples', 'root', 'root',
                                              'MySQL', '3306', 'db', 'MySQL'))

        exist = os.path.exists(os.path.join(DATA_DIR, 'shared',
                                            'ontopmaterialize_mysql.nt'))
        self.assertTrue(exist)
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared', 'ontopmaterialize_mysql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        ontop.stop()

    def test_ontopmaterialize_postgresql(self):
        ontop = OntopMaterialize(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'ontopmaterialize_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(ontop.execute_mapping('mapping.r2rml.ttl',
                                              'ontopmaterialize_postgresql.nt',
                                              'ntriples', 'root', 'root',
                                              'PostgreSQL', '5432', 'db',
                                              'PostgreSQL'))

        exist = os.path.exists(os.path.join(DATA_DIR, 'shared',
                                            'ontopmaterialize_postgresql.nt'))
        self.assertTrue(exist)
        g = Graph()
        g.parse(os.path.join(DATA_DIR, 'shared',
                             'ontopmaterialize_postgresql.nt'),
                format='ntriples')
        self.assertEqual(len(g), 3)

        ontop.stop()

    def test_ontopendpoint_mysql(self):
        QUERY='PREFIX foaf: <http://xmlns.com/foaf/0.1/> ' + \
              'CONSTRUCT WHERE { ?s foaf:name ?o1 . }'
        ontop = OntopVirtualize(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'ontopendpoint_mysql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(ontop.execute_mapping('mapping.r2rml.ttl',
                                              'ontopendpoint_mysql.nt',
                                              'ntriples', 'root', 'root',
                                              'MySQL', '3306', 'db', 'MySQL'))
        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, ontop.endpoint, ontop.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3)

        ontop.stop()

    def test_ontopendpoint_postgresql(self):
        QUERY='PREFIX foaf: <http://xmlns.com/foaf/0.1/> ' + \
              'CONSTRUCT WHERE { ?s foaf:name ?o1 . }'
        ontop = OntopVirtualize(DATA_DIR, CONFIG_DIR, False)
        try:
            os.remove(os.path.join(DATA_DIR, 'shared',
                                   'ontopendpoint_postgresql.nt'))
        except FileNotFoundError:
            pass
        self.assertTrue(ontop.execute_mapping('mapping.r2rml.ttl',
                                              'ontopendpoint_postgresql.nt',
                                              'ntriples', 'root', 'root',
                                              'PostgreSQL', '5432', 'db',
                                              'PostgreSQL'))
        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, ontop.endpoint, ontop.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3)

        ontop.stop()

class TriplestoreTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def tearDownClass(cls):
        manager.stop_all()

    def test_virtuoso(self):
        QUERY='PREFIX foaf: <http://xmlns.com/foaf/0.1/> ' + \
              'CONSTRUCT WHERE { ?s foaf:name ?o1 . }'
        virtuoso = Virtuoso(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(virtuoso.initialization())
        self.assertTrue(virtuoso.wait_until_ready())

        # Check if web interface is up
        r = requests.get('http://localhost:8890')
        self.assertEqual(r.status_code, 200)
        r.raise_for_status()

        # Check if SPARQL endpoint works
        r = requests.get('http://localhost:8890/sparql/'
                         '?default-graph-uri=&query=CONSTRUCT+WHERE+'
                         '%7B%0D%0A++%3Fs+%3Fp+%3Fo.%0D%0A%7D%0D%0ALIMIT+100'
                         '&format=text%2Fplain')
        self.assertEqual(r.status_code, 200)
        r.raise_for_status()

        # Check if iSQL is up, HTTP is unsupported on the iSQL port
        # so the connection will be closed without a response
        with self.assertRaises(requests.exceptions.ConnectionError) as e:
            r = requests.get('http://localhost:1111')
            r.raise_for_status()
        self.assertTrue('Connection aborted' in str(e.exception))

        # Test load RDF
        self.assertTrue(virtuoso.load('student.nt'))

        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, virtuoso.endpoint, virtuoso.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3)

        # RDF is dropped when container is stopped
        virtuoso.stop()

        # Virtuoso is already initialized only wait for it to be ready
        virtuoso = Virtuoso(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(virtuoso.wait_until_ready())

        # Verify removed data
        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, virtuoso.endpoint, virtuoso.headers)
        self.assertTrue('# Empty NT' in results)

        # Test if we can now reload our RDF
        self.assertTrue(virtuoso.load_parallel('stude*.nt', 4))

        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, virtuoso.endpoint, virtuoso.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3)

        virtuoso.stop()

    def test_fuseki(self):
        QUERY='PREFIX foaf: <http://xmlns.com/foaf/0.1/> ' + \
              'CONSTRUCT WHERE { ?s foaf:name ?o1 . }'
        fuseki = Fuseki(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(fuseki.initialization())
        self.assertTrue(fuseki.wait_until_ready())

        # Load RDF
        self.assertTrue(fuseki.load('student.nt'))

        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, fuseki.endpoint, fuseki.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3)

        fuseki.stop()

        # Verify removed data
        fuseki = Fuseki(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(fuseki.wait_until_ready())

        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, fuseki.endpoint, fuseki.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 0)

        # Test if we can now reload our RDF
        self.assertTrue(fuseki.load('student.nt'))

        # Verify loaded data
        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, fuseki.endpoint, fuseki.headers)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 3)

        fuseki.stop()

    def test_query_execute(self):
        QUERY = 'CONSTRUCT WHERE { ?s ?p ?o. } LIMIT 100'
        DBPEDIA_SPARQL = 'https://dbpedia.org/sparql'

        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q._execute(QUERY, DBPEDIA_SPARQL)
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 100)

    def test_query_execute_and_save(self):
        QUERY = 'CONSTRUCT WHERE { ?s ?p ?o. } LIMIT 100'
        DBPEDIA_SPARQL = 'https://dbpedia.org/sparql'

        q = Query(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(q.execute_and_save(QUERY, DBPEDIA_SPARQL, 'results.nt',
                                           {}))
        with open(os.path.join(DATA_DIR, 'shared', 'results.nt'), 'r') as f:
            results = list(filter(None, f.read().split('\n')))
            self.assertEqual(len(results), 100)

    def test_query_execute_from_file(self):
        QUERY_FILE = 'spo100.q'
        DBPEDIA_SPARQL = 'https://dbpedia.org/sparql'

        q = Query(DATA_DIR, CONFIG_DIR, False)
        results = q.execute_from_file(QUERY_FILE, DBPEDIA_SPARQL, {})
        results = list(filter(None, results.split('\n')))
        self.assertEqual(len(results), 100)

    def test_query_execute_from_file_and_save(self):
        QUERY_FILE = 'spo100.q'
        DBPEDIA_SPARQL = 'https://dbpedia.org/sparql'

        q = Query(DATA_DIR, CONFIG_DIR, False)
        self.assertTrue(q.execute_from_file_and_save(QUERY_FILE, DBPEDIA_SPARQL,
                                                     'results.nt', {}))
        with open(os.path.join(DATA_DIR, 'shared', 'results.nt'), 'r') as f:
            results = list(filter(None, f.read().split('\n')))
            self.assertEqual(len(results), 100)

class IntegrationTests(unittest.TestCase):
    def setUp(self):
        warnings.filterwarnings(action='ignore', message='unclosed',
                                category=ResourceWarning)

    @classmethod
    def tearDownClass(cls):
        manager.stop_all()

    def test_executor_list(self):
        executor = Executor(os.path.join(DATA_DIR, 'test-cases'), False)
        cases = executor.list()
        iris = [x['data']['@id'] for x in cases]
        BASE_URL = 'http://example.com/test-cases'
        e = [f'{BASE_URL}/rmlmapper/fuseki/transform-csv-to-ntriples',
             f'{BASE_URL}/rmlmapper/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/rmlmapper/fuseki/transform-postgresql-to-ntriples',
             f'{BASE_URL}/rmlmapper/virtuoso/transform-csv-to-ntriples',
             f'{BASE_URL}/rmlmapper/virtuoso/transform-mysql-to-ntriples',
             f'{BASE_URL}/rmlmapper/virtuoso/transform-postgresql-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/fuseki/transform-csv-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/fuseki/transform-postgresql-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/virtuoso/transform-csv-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/virtuoso/transform-mysql-to-ntriples',
             f'{BASE_URL}/sdmrdfizer/virtuoso/transform-postgresql-to-ntriples',
             f'{BASE_URL}/morphkgc/fuseki/transform-csv-to-ntriples',
             f'{BASE_URL}/morphkgc/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/morphkgc/fuseki/transform-postgresql-to-ntriples',
             f'{BASE_URL}/morphkgc/virtuoso/transform-csv-to-ntriples',
             f'{BASE_URL}/morphkgc/virtuoso/transform-mysql-to-ntriples',
             f'{BASE_URL}/morphkgc/virtuoso/transform-postgresql-to-ntriples',
             f'{BASE_URL}/ontopmaterialize/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/ontopmaterialize/fuseki/'
             'transform-postgresql-to-ntriples',
             f'{BASE_URL}/ontopmaterialize/virtuoso/'
             'transform-mysql-to-ntriples',
             f'{BASE_URL}/ontopmaterialize/virtuoso/'
             'transform-postgresql-to-ntriples',
             f'{BASE_URL}/ontopendpoint/fuseki/transform-mysql-to-ntriples',
             f'{BASE_URL}/ontopendpoint/fuseki/'
             'transform-postgresql-to-ntriples']
        # We expect 24 cases
        self.assertEqual(len(iris), 24)
        self.assertListEqual(sorted(e), sorted(iris))

    def test_executor_stats(self):
        path = os.path.join(DATA_DIR, 'test-cases', 'rmlmapper', 'fuseki')
        executor = Executor(path, False)

        # Limit to 3 cases to speed up tests
        for case in executor.list():
            directory = case['directory']

            # Clean case
            executor.clean(case)

            # Run case
            self.assertTrue(executor.run(case, 0.1, 1, False, True))

            # Generate stats
            executor.stats(case)

            # Verify stats
            aggregated_path = os.path.join(directory, 'results',
                                           'aggregated.json')
            self.assertTrue(os.path.exists(aggregated_path))
            with open(aggregated_path, 'r') as f:
                metrics = json.load(f)
                self.assertIn('stats', metrics)

    def test_executor_run(self):
        path = os.path.join(DATA_DIR, 'test-cases')
        executor = Executor(path, False)

        for case in executor.list():
            # Clean case
            executor.clean(case)

            # Check if all files are removed
            for step in case['data']['steps']:
                directory = case['directory']
                resource = step['resource'].lower().replace('_', '')
                log_file = os.path.join(directory, 'results', 'run_1', resource,
                                        LOGS_FILE_NAME)
                checkpoint_file = os.path.join(directory, CHECKPOINT_FILE_NAME)
                metrics_file = os.path.join(directory, 'results', 'run_1',
                                            resource, METRICS_FILE_NAME)

                self.assertFalse(os.path.exists(log_file))
                self.assertFalse(os.path.exists(metrics_file))
                self.assertFalse(os.path.exists(checkpoint_file))

            # Run case
            self.assertTrue(executor.run(case, 0.1, 1, False, True))

            # Check if all files are generated
            for step in case['data']['steps']:
                directory = case['directory']
                resource = step['resource'].lower().replace('_', '')
                log_file = os.path.join(directory, 'results', 'run_1', resource,
                                        LOGS_FILE_NAME)
                checkpoint_file = os.path.join(directory, CHECKPOINT_FILE_NAME)
                metrics_file = os.path.join(directory, 'results', 'run_1',
                                            METRICS_FILE_NAME)

                # Verify if log, checkpoint, metrics files exists
                # Log files are expected to contain at least 1 line,
                # Metrics files at least 3: START, MEASUREMENT, STOP types, and
                # Checkpoint files contain a single line with the timestamp
                self.assertTrue(os.path.exists(log_file))
                with open(log_file) as f:
                    self.assertTrue(len(f.readlines()) > 0)
                self.assertTrue(os.path.exists(metrics_file))
                with open(metrics_file) as f:
                    lines = f.readlines()

                    for l in lines:
                        m = json.loads(l)
                        if m['type'] == METRIC_TYPE_START or \
                            m['type'] == METRIC_TYPE_STOP or \
                            m['type'] == METRIC_TYPE_INIT or \
                            m['type'] == METRIC_TYPE_EXIT:
                            for key in ['interval', 'resource', 'time', 'type']:
                                self.assertIn(key, m)
                                self.assertIsNotNone(m[key])
                        elif m['type'] == METRIC_TYPE_MEASUREMENT:
                            for key in ['interval', 'resource', 'time', 'type',
                                        'cpu_total_time', 'cpu_user_time',
                                        'cpu_system_time', 'memory_total_size',
                                        'io', 'network']:
                                self.assertIn(key, m)
                                self.assertIsNotNone(m[key])

                                if key == 'io':
                                    for key2 in ['device', 'total_size_read',
                                                 'total_size_write',
                                                 'total_size_discard',
                                                 'number_of_read',
                                                 'number_of_write',
                                                 'number_of_discard']:
                                        for entry in m['io']:
                                            self.assertIn(key2, entry)
                                            self.assertIsNotNone(entry[key2])

                                if key == 'network':
                                    for key3 in ['device',
                                                 'total_size_received',
                                                 'total_size_transmitted',
                                                 'number_of_packets_received',
                                                 'number_of_packets_transmitted',
                                                 'number_of_errors_received',
                                                 'number_of_errors_transmitted',
                                                 'number_of_drops_received',
                                                 'number_of_drops_transmitted']:
                                        for entry in m['network']:
                                            self.assertIn(key3, entry)
                                            self.assertIsNotNone(entry[key3])
                        else:
                            raise ValueError('Unknown metric type: '
                                             f'"{m["type"]}"')

                self.assertTrue(os.path.exists(checkpoint_file))
                with open(checkpoint_file) as f:
                    self.assertEqual(len(f.readlines()), 1)

if __name__ == '__main__':
    # SELinux causes weird permission denied issues, warn users
    try:
        response = subprocess.check_output('getenforce')
        if response.decode().strip() != 'Permissive':
            print('SELinux must be set to "permissive" to allow containers '
                  'accessing files in mounted directories', file=sys.stderr)
            sys.exit(-1)
    except subprocess.CalledProcessError:
        pass
    except FileNotFoundError:
        pass

    if len(manager.list_all()) != 0:
        print('Stopping all containers before starting tests...')
        manager.stop_all()

    if len(sys.argv) > 1:
        unittest.main()
    else:
        print('*' * 3 + ' UNIT TESTS ' + '*' * 3)
        unittest.main(defaultTest='UnitTests', exit=False)
        print('*' * 3 + ' FILE TESTS ' + '*' * 3)
        unittest.main(defaultTest='FileTests', exit=False)
        print('*' * 3 + ' RDB TESTS ' + '*' * 3)
        unittest.main(defaultTest='RDBTests', exit=False)
        print('*' * 3 + ' TRIPLE STORE TESTS ' + '*' * 3)
        unittest.main(defaultTest='TriplestoreTests', exit=False)
        print('*' * 3 + 'INTEGRATION TESTS' + '*' * 3)
        unittest.main(defaultTest='IntegrationTests')
